{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build My Own Artificial Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What’s a Neural Network?\n",
    "### Most introductory texts to Neural Networks brings up brain analogies when describing them. Without delving into brain analogies, I find it easier to simply describe Neural Networks as a mathematical function that maps a given input to a desired output.\n",
    "### Neural Networks consist of the following components\n",
    "### - An input layer, x\n",
    "### - An arbitrary amount of hidden layers\n",
    "### - An output layer, ŷ\n",
    "### - A set of weights and biases between each layer, W and b\n",
    "### - A choice of activation function for each hidden layer, σ. In this tutorial, we’ll use a Sigmoid activation function.\n",
    "### The diagram below shows the architecture of a 2-layer Neural Network (note that the input layer is typically excluded when counting the number of layers in a Neural Network)\n",
    "\n",
    "\n",
    "%%html\n",
    "<img src = \"https://miro.medium.com/max/1000/1*sX6T0Y4aa3ARh7IBS_sdqw.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "### The output ŷ of a simple 2-layer Neural Network is:\n",
    "\n",
    "%%html\n",
    "<img src = \"https://miro.medium.com/max/710/1*E1_l8PGamc2xTNS87XGNcA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You might notice that in the equation above, the weights W and the biases b are the only variables that affects the output ŷ.\n",
    "### Naturally, the right values for the weights and biases determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as training the Neural Network.\n",
    "### Each iteration of the training process consists of the following steps:\n",
    "### - Calculating the predicted output ŷ, known as feedforward\n",
    "### - Adjusting the weights and biases, known as backpropagation\n",
    "### The sequential graph below illustrates the process.\n",
    "\n",
    "%%html\n",
    "<img src = \"https://miro.medium.com/max/1400/1*CEtt0h8Rss_qPu7CyqMTdQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Formulas : \n",
    "### --> sigmoid                    = 1/(1+np.exp(-x))      \n",
    "### --> sigmoid_derivative = x*(1-x)\n",
    "### --> error = output - predicted_output   \n",
    "### --> Adjust_weights_by = error.input.sigmoid_derivative    \n",
    "### Note - In above formulas np is numpy , exp is exponential and ( . ) is dot product and moreover there are many formulas and types of activation functions and loss functions ( i.e error ) but in this tutorial I will use this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method - 01 \n",
    "### ( This Method is Just for Clearing Concepts from Next Methods we wil build it systematically )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : 0.49641003190272537\n",
      "Error : 0.008584525653247155\n",
      "Error : 0.005789459862507812\n",
      "Error : 0.004629176776769985\n",
      "Error : 0.0039587652802736475\n",
      "Error : 0.0035101225678616744\n",
      "\n",
      "Output after Training\n",
      "[[0.00260572]\n",
      " [0.99672209]\n",
      " [0.99701711]\n",
      " [0.00386759]]\n"
     ]
    }
   ],
   "source": [
    "# Impoting required Libraries\n",
    "import numpy as np\n",
    "\n",
    "# Input Data\n",
    "x = np.array([[0,0,1],\n",
    "             [0,1,1],\n",
    "             [1,0,1],\n",
    "             [1,1,1]])\n",
    "\n",
    "# Output Data\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "\n",
    "# Sigmoid function (One of the type of Activation function)\n",
    "def sigmoid(x , deriv = False):\n",
    "    \n",
    "    if deriv == True:\n",
    "        return (x*(1-x))     # We will active this in Back Propogation\n",
    "    \n",
    "    return 1/(1+np.exp(-x))  # We will active this in Forward Propogation\n",
    "\n",
    "\n",
    "# Seed\n",
    "np.random.seed(1)    # For Debugging\n",
    "\n",
    "# Synapses\n",
    "syn0 = 2*np.random.random((x.shape[1],4)) -1  # Here first argument is no. of input cols & second is no. nodes\n",
    "syn1 = 2*np.random.random(y.shape) - 1        # Here argument is shape of output\n",
    "\n",
    "# Training\n",
    "\n",
    "for j in list(range(60000)):\n",
    "    \n",
    "    # Layers\n",
    "    l0 = x                             # Input layer\n",
    "    l1 = sigmoid(np.dot(l0, syn0))     # Hidden layer\n",
    "    l2 = sigmoid(np.dot(l1, syn1))     # Output layer\n",
    "    \n",
    "    # Backpropagation\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if j%10000 == 0:    # This will print the error rate in every 10000 training_iterations\n",
    "        print('Error : ' + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    # Calculate deltas\n",
    "    l2_delta = l2_error * sigmoid(l2 , deriv = True)\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    l1_delta = l1_error * sigmoid(l1 , deriv = True)\n",
    "    \n",
    "    # Update our synapses\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    \n",
    "print()\n",
    "print('Output after Training')\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method - 02\n",
    "### ( In this method we will consider that there is no hidden layers inputs is directly connected to output )\n",
    "### ( In Next method we will drive into hidden layers too )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "\n",
      "Error : 0.0032161293267241654\n",
      "\n",
      "Outputs after training\n",
      "[[0.00390234]\n",
      " [0.99681554]\n",
      " [0.99740397]\n",
      " [0.00318168]]\n",
      "\n",
      "Synaptic weights after training: \n",
      "[[11.49345763]\n",
      " [-0.2048909 ]\n",
      " [-5.54227641]]\n",
      "\n",
      "Let's Predict...!\n",
      "Input 1: 1\n",
      "Input 2: 0\n",
      "Input 3: 1\n",
      "New situation: input data =  1 0 1\n",
      "Output data: \n",
      "[0.99740399]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Seed the random number generator\n",
    "        np.random.seed(1)    # For debugging\n",
    "\n",
    "        # Set synaptic weights to a 3x1 matrix, with values from -1 to 1 and mean 0\n",
    "        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "        \n",
    "    def sigmoid(self, x, deriv = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        The derivative of the sigmoid function used to\n",
    "        calculate necessary weight adjustments (Back Propagation)\n",
    "        \"\"\"\n",
    "        if deriv == True:\n",
    "            return x * (1 - x)\n",
    "        \n",
    "        \"\"\"\n",
    "        Takes in weighted sum of the inputs and normalizes\n",
    "        them through between 0 and 1 through a sigmoid function (Forward Propagation)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        \n",
    "    def train(self, training_inputs, training_outputs, training_iterations):\n",
    "        \n",
    "        \"\"\"\n",
    "        We train the model through trial and error, adjusting the\n",
    "        synaptic weights each time to get a better result\n",
    "        \"\"\"\n",
    "        for neural_network in range(training_iterations):\n",
    "            \n",
    "            # Pass training set through the neural network\n",
    "            output = self.think(training_inputs)\n",
    "            \n",
    "            # Calculate the error rate\n",
    "            error = training_outputs - output\n",
    "\n",
    "            # Multiply error by input and gradient of the sigmoid function\n",
    "            # Less confident weights are adjusted more through the nature of the function\n",
    "            #np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "            \n",
    "            adjustments = np.dot(training_inputs.T, error * self.sigmoid(output , deriv = True)) \n",
    "            \n",
    "            # Adjust synaptic weights\n",
    "            self.synaptic_weights += adjustments\n",
    "            \n",
    "        print()\n",
    "        print('Error : ' + str(np.mean(np.abs(error))))\n",
    "        print()\n",
    "        print('Outputs after training')\n",
    "        print(output)\n",
    "\n",
    "    def think(self, inputs):\n",
    "        \"\"\"\n",
    "        Pass inputs through the neural network to get output\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = inputs.astype(float)\n",
    "        output = self.sigmoid(np.dot(inputs, self.synaptic_weights))\n",
    "        return output\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize the single neuron neural network\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"Random starting synaptic weights: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    # The training set, with 4 examples consisting of 3 input values and 1 output value\n",
    "    training_inputs = np.array([[0,0,1],\n",
    "                                [1,1,1],\n",
    "                                [1,0,1],\n",
    "                                [0,1,1]])\n",
    "\n",
    "    training_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "    # Train the neural network\n",
    "    neural_network.train(training_inputs, training_outputs, 60000)\n",
    "\n",
    "    print()\n",
    "    print(\"Synaptic weights after training: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "    \n",
    "    print()\n",
    "    print(\"Let's Predict...!\")\n",
    "    A = int(input(\"Input 1: \"))\n",
    "    B = int(input(\"Input 2: \"))\n",
    "    C = int(input(\"Input 3: \"))\n",
    "    \n",
    "    print(\"New situation: input data = \", A, B, C)\n",
    "    print(\"Output data: \")\n",
    "    print(neural_network.think(np.array([A, B, C])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method - 03\n",
    "### ( In this method we will consider that there is 1 hidden layer with 4 nodes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n",
      "\n",
      "Error : 0.005002695580586832\n",
      "\n",
      "Outputs after training\n",
      "[[0.00510229]\n",
      " [0.99437164]\n",
      " [0.99493875]\n",
      " [0.00421887]]\n",
      "\n",
      "Synaptic weights after training: \n",
      "[[-2.45237664  4.24246073 -4.51602703  0.20084556]\n",
      " [-0.37852498 -0.46226218  0.03227375 -0.22607559]\n",
      " [ 0.70578881 -1.40848121  1.82311171  0.44840252]]\n",
      "[[-2.73124389]\n",
      " [ 6.00597609]\n",
      " [-5.78228268]\n",
      " [ 0.57952083]]\n",
      "\n",
      "Let's Predict...!\n",
      "Input 1: 0\n",
      "Input 2: 1\n",
      "Input 3: 1\n",
      "New situation: input data =  0 1 1\n",
      "Output data: \n",
      "[0.00421865]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Seed the random number generator\n",
    "        np.random.seed(1)    # For debugging\n",
    "\n",
    "        # Set synaptic weights to a 3x1 matrix,\n",
    "        # with values from -1 to 1 and mean 0\n",
    "        #self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "        self.synaptic_weights0 = 2*np.random.random((3,4)) - 1   # Here first argument is no. of input cols & second is no. nodes\n",
    "        self.synaptic_weights1 = 2*np.random.random((4,1)) - 1   # Here argument is shape of output\n",
    "\n",
    "    def sigmoid(self, x, deriv = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        The derivative of the sigmoid function used to\n",
    "        calculate necessary weight adjustments\n",
    "        \"\"\"\n",
    "        if deriv == True:\n",
    "            return x * (1 - x)\n",
    "        \n",
    "        \"\"\"\n",
    "        Takes in weighted sum of the inputs and normalizes\n",
    "        them through between 0 and 1 through a sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def train(self, training_inputs, training_outputs, training_iterations):\n",
    "        \"\"\"\n",
    "        We train the model through trial and error, adjusting the\n",
    "        synaptic weights each time to get a better result\n",
    "        \"\"\"\n",
    "        for neural_network in range(training_iterations):\n",
    "            # Pass training set through the neural network\n",
    "            hidden = self.think(training_inputs)[0]\n",
    "            output = self.think(training_inputs)[1]\n",
    "            inputs = self.think(training_inputs)[2]\n",
    "    \n",
    "            # Calculate the error rate\n",
    "            error = training_outputs - output\n",
    "\n",
    "            # Multiply error by input and gradient of the sigmoid function\n",
    "            # Less confident weights are adjusted more through the nature of the function\n",
    "            \n",
    "            output_delta = error * self.sigmoid(output, deriv = True)\n",
    "            error_hidden = output_delta.dot(self.synaptic_weights1.T)    # hidden layer error\n",
    "            hidden_delta = error_hidden * self.sigmoid(hidden, deriv = True)\n",
    "            \n",
    "            adjustments1 = np.dot(hidden.T, output_delta)\n",
    "            adjustments0 = np.dot(inputs.T, hidden_delta)\n",
    "            \n",
    "            # Adjust synaptic weights\n",
    "            self.synaptic_weights1 += adjustments1\n",
    "            self.synaptic_weights0 += adjustments0\n",
    "            \n",
    "        print()\n",
    "        print('Error : ' + str(np.mean(np.abs(error))))\n",
    "        print()\n",
    "        print('Outputs after training')\n",
    "        print(output)\n",
    "        \n",
    "    def think(self, inputs):\n",
    "        \"\"\"\n",
    "        Pass inputs through the neural network to get output\n",
    "        \"\"\"\n",
    "        inputs = inputs.astype(float)\n",
    "        hidden = self.sigmoid(np.dot(inputs, self.synaptic_weights0))\n",
    "        output = self.sigmoid(np.dot(hidden, self.synaptic_weights1))\n",
    "        \n",
    "        return hidden , output , inputs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize the single neuron neural network\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"Random starting synaptic weights: \")\n",
    "    print(neural_network.synaptic_weights0)\n",
    "    print(neural_network.synaptic_weights1)\n",
    "\n",
    "    # The training set, with 4 examples consisting of 3 input values and 1 output value\n",
    "    training_inputs = np.array([[0,0,1],\n",
    "                                [1,1,1],\n",
    "                                [1,0,1],\n",
    "                                [0,1,1]])\n",
    "\n",
    "    training_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "    # Train the neural network\n",
    "    neural_network.train(training_inputs, training_outputs, 10000)\n",
    "\n",
    "    print()\n",
    "    print(\"Synaptic weights after training: \")\n",
    "    print(neural_network.synaptic_weights0)\n",
    "    print(neural_network.synaptic_weights1)\n",
    "    \n",
    "    print()\n",
    "    print(\"Let's Predict...!\")\n",
    "    A = int(input(\"Input 1: \"))\n",
    "    B = int(input(\"Input 2: \"))\n",
    "    C = int(input(\"Input 3: \"))\n",
    "    \n",
    "    print(\"New situation: input data = \", A, B, C)\n",
    "    print(\"Output data: \")\n",
    "    print(neural_network.think(np.array([A, B, C]))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...END..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
